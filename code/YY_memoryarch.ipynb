{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory Systems\n",
    "\n",
    "* Large class defined by memory model\n",
    "  * And thus, the programming model\n",
    "* In shared-memory programming, \n",
    "  * parallel _threads_ \n",
    "  * exchange information through reads and writes to shared memory.\n",
    "  * synchronization constructs  control sharing\n",
    "* Easy to use abstraction\n",
    "* Examples\n",
    "  * OpenMP, Java, pthreads, C++ threads\n",
    "  \n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/uma.gif\" width=\"512\" title=\"Uniform Shared Memory System\" />\n",
    "\n",
    "### SMP = Symmetric Multiprocess\n",
    "\n",
    "* Simplest form of shared-memory MIMD system in which \n",
    " * All processors can address all memory\n",
    " * Symmetric performance (latency and throughput) from all processors to all memory addresses\n",
    "* SMPs have scaling limits\n",
    "  * physics makes it difficult to maintain symmetry as we add cores to processors or processors to machines.\n",
    "  \n",
    "Few (or no) machines end up being symmetric in practice.  Let's consider:\n",
    "  * a two processor machine with symmetric access to memory\n",
    "  * each processor has 4-cores with symmetric access to an L3 cache\n",
    "As the program runs data ends up cached in memory, L3 and higher-level caches.  Processors have assymetric access to data in an L3 cache.  Cores have assymetric access to data in core-private caches.\n",
    "\n",
    "Highly-optimized programs must consider locality between compute and data even in \"symmetric\" architectures.\n",
    "\n",
    "### NUMA Machines\n",
    "\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/numa.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "* Shared memory MIMD systems\n",
    "  * Latency and bandwidth to physical memory differs by address and location\n",
    "* Same programming semantics as SMP\n",
    "\n",
    "The big hazard of shared memory programming is that tools (OpenMP, pthreads, Java threads) and programmers typically treat machines as if they were SMPs when in fact they are all NUMA.  Alas, programming for NUMA is hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Memory Systems\n",
    "\n",
    "TODO RB fix image.\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/numa.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "MIMD machines in which the processing units have their own private memory.  The only way to share data among nodes is to moved it explicitly from one memory to another.\n",
    "* _Message passing_\n",
    "  * A programming pattern in which parallel nodes intermix computation with sending and receiving data from other nodes.\n",
    "  * MPI (message-passing interface) is the standard programming environment to implement this pattern.\n",
    "* Cloud frameworks create programming environments that map computations to distributed memory\n",
    "  * Hadoop!(Map/Reduce): data-parallel programs over file I/O\n",
    "  * Spark: data-parallel programs over memory-resident partitioned data structures\n",
    "  \n",
    "  \n",
    "### Hybrid Architectures\n",
    "\n",
    "TODO RB fix image.\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/numa.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "Hierarchical combination of multiple architectures.\n",
    "* Simple cloud or simple cluster \n",
    "  * distributed memory system consisting of\n",
    "  * SMP or NUMA nodes.\n",
    "* HPC or machine learning cloud\n",
    "  * distributed memory system consisting of\n",
    "  * SMP or NUMA nodes\n",
    "  * that have GPU, FPGA, Sunway, Intel MIC accelerators\n",
    "  \n",
    "The key challenge in modern parallel programming is efficiently mapping problems into codes that run efficiently on hybrid architectures.\n",
    "* The HPC approach is to write an MPI program that runs OpenMP programs on each node and makes calls to accelerators\n",
    "  * This is expensive and hard to maintain\n",
    "  * But a general solution and required to utilize hardware well\n",
    "* Machine learning frameworks have recently made great strides toward transparent support for hybrid systems\n",
    "  * PyTorch: NUMA+GPU\n",
    "  * MxNet: cloud+NUMA+GPU\n",
    "  * a great path forward for compute-dense iterative models\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
